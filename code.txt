

===== main.cu =====
// main.cu
// Test infrastructure for comparing scan algorithms

#include <iostream>
#include <cuda_runtime.h>
#include <vector>
#include <utils.cuh>

#define CHECK_CUDA(val) check((val), #val, __FILE__, __LINE__)
void check(cudaError_t err, const char* const func, const char* const file,
           const int line)
{
    if (err != cudaSuccess)
    {
        std::cerr << "CUDA Error at: " << file << ":" << line << std::endl;
        std::cerr << cudaGetErrorString(err) << " " << func << std::endl;
        std::exit(EXIT_FAILURE);
    }
}

// ============================================================================
// HELPER FUNCTIONS
// ============================================================================

void Initialization(int *buffer, int *gold, int n)
{
    for (int i = 0; i < n; ++i)
    {
        buffer[i] = 1;
    }

    gold[0] = buffer[0];
    for (int i = 1; i < n; ++i)
    {
        gold[i] = gold[i - 1] + buffer[i];
    }
}

void VerifyScan(const int* output, const int* gold, int n)
{
    for (int i = 0; i < n; ++i)
    {
        if (output[i] != gold[i])
        {
            std::cerr << "Mismatch at index " << i << ": "
                      << "got " << output[i] << ", expected " << gold[i] << std::endl;
            std::exit(EXIT_FAILURE);
        }
    }
}

float GetPeakBandwidth()
{
    int device;
    CHECK_CUDA(cudaGetDevice(&device));
    
    int memory_clock_khz;
    int memory_bus_width_bits;
    
    CHECK_CUDA(cudaDeviceGetAttribute(&memory_clock_khz, 
                                      cudaDevAttrMemoryClockRate, device));
    CHECK_CUDA(cudaDeviceGetAttribute(&memory_bus_width_bits, 
                                      cudaDevAttrGlobalMemoryBusWidth, device));
    
    float peak_bandwidth_gbs = 2.0f * memory_clock_khz * 
                               (memory_bus_width_bits / 8.0f) / 1e6f;
    
    return peak_bandwidth_gbs;
}

// ============================================================================
// WARP AND BLOCK SCAN PRIMITIVES
// ============================================================================

static __device__ __forceinline__ int WarpScan(int value)
{
    int lane = threadIdx.x % warpSize;
    for (int offset = 1; offset < warpSize; offset *= 2) {
        int tmp = __shfl_up_sync(0xFFFFFFFF, value, offset);
        if (lane >= offset) {
            value += tmp;
        }
    }
    return value;
}

template<int BLOCK_SIZE>
static __device__ __forceinline__ int BlockScan(int value)
{
    int wid = threadIdx.x / warpSize;
    int lane = threadIdx.x % warpSize;
    constexpr int num_warps = (BLOCK_SIZE + 31) / 32;

    int warp_scan = WarpScan(value);

    __shared__ int smem[num_warps];
    if (lane == warpSize-1) {
        smem[wid] = warp_scan;
    }
    __syncthreads();

    if (wid == 0) {
        int tmp = (threadIdx.x < num_warps) ? smem[threadIdx.x] : 0;
        tmp = WarpScan(tmp);
        if (threadIdx.x < num_warps) {
            smem[threadIdx.x] = tmp;
        }
    }
    __syncthreads();

    int warp_prefix = (wid > 0) ? smem[wid - 1] : 0;
    int block_scan = warp_scan + warp_prefix;
    return block_scan;
}

template<int BLOCK_SIZE>
static __device__ __forceinline__ int BlockScanExclusive(int value)
{
    int inclusive = BlockScan<BLOCK_SIZE>(value);
    return inclusive - value;
}

// ============================================================================
// KERNELS
// ============================================================================

template<int BLOCK_SIZE>
__global__ void ScanBlocksWarp(
    const int* __restrict__ input,
    int* __restrict__ output,
    int n,
    int* block_sums)
{
    int gid = blockIdx.x * blockDim.x + threadIdx.x;

    int value = (gid < n) ? input[gid] : 0;
    value = BlockScan<BLOCK_SIZE>(value);

    if (gid < n) {
        output[gid] = value;
    }

    if (threadIdx.x == BLOCK_SIZE - 1 && block_sums != nullptr) {
        block_sums[blockIdx.x] = value;
    }
}



template<int BLOCK_SIZE>
__global__ void ScanBlocksSMEM(
    const int* __restrict__ input,
    int* __restrict__ output,
    int n,
    int* block_sums)
{
    int gid = blockIdx.x * blockDim.x + threadIdx.x;
    int tid = threadIdx.x;

    __shared__ int smem[2][BLOCK_SIZE];

    int wbuf = 0;
    int rbuf = 1;
    smem[wbuf][tid] = (gid < n) ? input[gid] : int(0);
    __syncthreads();

    for (int offset = 1; offset < BLOCK_SIZE; offset *= 2) {
        wbuf = 1 - wbuf;
        rbuf = 1 - wbuf;
        if (tid >= offset) {
            smem[wbuf][tid] = smem[rbuf][tid - offset] + smem[rbuf][tid];
        } else {
            smem[wbuf][tid] = smem[rbuf][tid];
        }
        __syncthreads();
    }

    if (gid < n) {
        output[gid] = smem[wbuf][tid];
    }
    
    if (tid == BLOCK_SIZE - 1 && block_sums != nullptr) {
        block_sums[blockIdx.x] = smem[wbuf][BLOCK_SIZE - 1];
    }
}

__global__ void AddBlockSums(int* output, int n, const int* scanned_block_sums)
{
    int gid = blockIdx.x * blockDim.x + threadIdx.x;

    if (blockIdx.x > 0 && gid < n) {
        output[gid] += scanned_block_sums[blockIdx.x - 1];
    }
}

// ============================================================================
// MULTI-KERNEL SCAN ALGORITHMS
// ============================================================================

template<int BLOCK_SIZE>
void ScanMultiKernelBaseline(int* d_input, int* d_output, int n)
{
    int num_blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;

    if (num_blocks == 1) {
        ScanBlocksSMEM<BLOCK_SIZE><<<1, BLOCK_SIZE>>>(d_input, d_output, n, nullptr);
        CHECK_CUDA(cudaGetLastError());
        return;
    }

    int* d_block_sums;
    int* d_block_sums_scanned;
    CHECK_CUDA(cudaMalloc(&d_block_sums, num_blocks * sizeof(int)));
    CHECK_CUDA(cudaMalloc(&d_block_sums_scanned, num_blocks * sizeof(int)));

    ScanBlocksSMEM<BLOCK_SIZE><<<num_blocks, BLOCK_SIZE>>>(d_input, d_output, n, d_block_sums);
    CHECK_CUDA(cudaGetLastError());

    ScanMultiKernelBaseline<BLOCK_SIZE>(d_block_sums, d_block_sums_scanned, num_blocks);
    
    AddBlockSums<<<num_blocks, BLOCK_SIZE>>>(d_output, n, d_block_sums_scanned);
    CHECK_CUDA(cudaGetLastError());

    CHECK_CUDA(cudaFree(d_block_sums));
    CHECK_CUDA(cudaFree(d_block_sums_scanned));
}

template<int BLOCK_SIZE>
void ScanMultiKernelWarp(int* d_input, int* d_output, int n)
{
    int num_blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;

    if (num_blocks == 1) {
        ScanBlocksWarp<BLOCK_SIZE><<<1, BLOCK_SIZE>>>(d_input, d_output, n, nullptr);
        CHECK_CUDA(cudaGetLastError());
        return;
    }

    int* d_block_sums;
    int* d_block_sums_scanned;
    CHECK_CUDA(cudaMalloc(&d_block_sums, num_blocks * sizeof(int)));
    CHECK_CUDA(cudaMalloc(&d_block_sums_scanned, num_blocks * sizeof(int)));

    ScanBlocksWarp<BLOCK_SIZE><<<num_blocks, BLOCK_SIZE>>>(d_input, d_output, n, d_block_sums);
    CHECK_CUDA(cudaGetLastError());

    ScanMultiKernelWarp<BLOCK_SIZE>(d_block_sums, d_block_sums_scanned, num_blocks);
    
    AddBlockSums<<<num_blocks, BLOCK_SIZE>>>(d_output, n, d_block_sums_scanned);
    CHECK_CUDA(cudaGetLastError());

    CHECK_CUDA(cudaFree(d_block_sums));
    CHECK_CUDA(cudaFree(d_block_sums_scanned));
}

// ============================================================================
// SINGLE-KERNEL SCAN ALGORITHMS (TO BE IMPLEMENTED)
// ============================================================================

void ScanChained(int* d_input, int* d_output, int n)
{
    // TODO: Implement chained scan
    // - Each block waits for previous block to finish
    // - Uses atomics for synchronization
    // - Expected to be slow due to serialization
    std::cerr << "ScanChained not yet implemented" << std::endl;
    std::exit(EXIT_FAILURE);
}

void ScanLookbackBlock(int* d_input, int* d_output, int n)
{
    // TODO: Implement decoupled lookback with single-thread lookback
    // - Each block scans independently
    // - Single thread per block does lookback
    // - Uses INVALID/AGGREGATE/PREFIX states
    std::cerr << "ScanLookbackBlock not yet implemented" << std::endl;
    std::exit(EXIT_FAILURE);
}

void ScanLookbackWarp(int* d_input, int* d_output, int n)
{
    // TODO: Implement decoupled lookback with warp-level lookback
    // - Entire warp cooperates on lookback (32x parallelism)
    // - Uses __ballot_sync and warp reductions
    // - Expected to be fastest
    std::cerr << "ScanLookbackWarp not yet implemented" << std::endl;
    std::exit(EXIT_FAILURE);
}

void ScanLookbackWarpCutoff(int* d_input, int* d_output, int n)
{
    // TODO: Implement warp lookback with cutoff optimization
    // - Same as ScanLookbackWarp but with cutoff
    // - Prevents worst-case deep lookback chains
    // - Fallback to serial scan after CUTOFF blocks
    std::cerr << "ScanLookbackWarpCutoff not yet implemented" << std::endl;
    std::exit(EXIT_FAILURE);
}

template<typename ScanFunc>
void RunTest(
    const char* label,
    ScanFunc scan_func,
    int n,
    float peak_bandwidth,
    int warmup_runs = 2,
    int timed_runs = 10)
{
    std::cout << "\n================================================" << std::endl;
    std::cout << "Testing: " << label << std::endl;
    std::cout << "================================================" << std::endl;

    // Initialize host memory
    int* h_input = new int[n];
    int* h_gold = new int[n];
    Initialization(h_input, h_gold, n);

    // Allocate device memory
    int* d_input;
    int* d_output;
    CHECK_CUDA(cudaMalloc(&d_input, n * sizeof(int)));
    CHECK_CUDA(cudaMalloc(&d_output, n * sizeof(int)));
    CHECK_CUDA(cudaMemcpy(d_input, h_input, n * sizeof(int), cudaMemcpyHostToDevice));

    // Warmup runs
    for (int i = 0; i < warmup_runs; ++i) {
        scan_func(d_input, d_output, n);
        CHECK_CUDA(cudaDeviceSynchronize());
    }
    // Validate first (fail fast)
    int* h_output = new int[n];
    CHECK_CUDA(cudaMemcpy(h_output, d_output, n * sizeof(int), cudaMemcpyDeviceToHost));
    VerifyScan(h_output, h_gold, n);

    // Timed runs
    cudaEvent_t start, stop;
    CHECK_CUDA(cudaEventCreate(&start));
    CHECK_CUDA(cudaEventCreate(&stop));

    std::vector<float> times;
    for (int i = 0; i < timed_runs; ++i) {
        CHECK_CUDA(cudaEventRecord(start));
        scan_func(d_input, d_output, n);
        CHECK_CUDA(cudaEventRecord(stop));
        CHECK_CUDA(cudaEventSynchronize(stop));

        float time_ms;
        CHECK_CUDA(cudaEventElapsedTime(&time_ms, start, stop));
        times.push_back(time_ms);
    }

    // Calculate statistics
    float min_time = times[0];
    float max_time = times[0];
    float sum_time = 0.0f;
    for (float t : times) {
        min_time = (t < min_time) ? t : min_time;
        max_time = (t > max_time) ? t : max_time;
        sum_time += t;
    }
    float avg_time = sum_time / timed_runs;

    // Calculate bandwidth
    size_t bytes = 2 * n * sizeof(int);
    float bandwidth_gbs = (bytes / 1e9f) / (avg_time / 1000.0f);
    float percent_peak = (bandwidth_gbs / peak_bandwidth) * 100.0f;

    // Print results
    std::cout << "Time (avg/min/max): " 
              << avg_time << " / " << min_time << " / " << max_time << " ms" << std::endl;
    std::cout << "Bandwidth: " << bandwidth_gbs << " GB/s (" 
              << percent_peak << "% of peak)" << std::endl;

    // Cleanup
    CHECK_CUDA(cudaEventDestroy(start));
    CHECK_CUDA(cudaEventDestroy(stop));
    CHECK_CUDA(cudaFree(d_input));
    CHECK_CUDA(cudaFree(d_output));
    delete[] h_input;
    delete[] h_output;
    delete[] h_gold;
}

// ============================================================================
// MAIN
// ============================================================================

int main(int argc, char** argv)
{
    size_t num_elements = size_t{1} << 30;

    if (argc >= 2) {
        int power = std::atoi(argv[1]);
        if (power < 1 || power > 30) {
            std::cerr << "Power must be between 1 and 30" << std::endl;
            return EXIT_FAILURE;
        }
        num_elements = size_t{1} << power;
    }

    std::cout << "Array size: " << num_elements << " elements (" 
              << (num_elements * sizeof(int)) / 1e9 << " GB)" << std::endl;

    float peak_bandwidth = GetPeakBandwidth();
    std::cout << "Device peak bandwidth: " << peak_bandwidth << " GB/s" << std::endl;

    // Run all tests
    RunTest("Multi-kernel Baseline (SMEM)", ScanMultiKernelBaseline, num_elements, peak_bandwidth);
    RunTest("Multi-kernel Warp Primitives", ScanMultiKernelWarp, num_elements, peak_bandwidth);

    return EXIT_SUCCESS;
}

===== scan_chained.cuh =====
// scan_chained.cuh
#pragma once

template<int BLOCK_SIZE>
__global__ void ScanChainedKernel(
    const int* __restrict__ input,
    int* __restrict__ output,
    int n,
    int* block_prefixes,
    int* block_status,
    int* g_block_counter)
{
    __shared__ int s_logical_idx;
    __shared__ int s_prefix;

    // Step 1: Dynamically claim logical block index
    if (threadIdx.x == 0) {
        s_logical_idx = atomicAdd(g_block_counter, 1);
    }
    __syncthreads();

    int logical_idx = s_logical_idx;
    int gid = logical_idx * BLOCK_SIZE + threadIdx.x;

    // Step 2: Load and scan block
    int value = (gid < n) ? input[gid] : 0;
    value = BlockScan<BLOCK_SIZE>(value);

    // Step 3: Single thread waits for previous block and fetches prefix
    if (threadIdx.x == BLOCK_SIZE - 1) {
        if (logical_idx == 0) {
            s_prefix = 0;
        } else {
            // Spin-wait for previous block
            while (atomicAdd(&block_status[logical_idx - 1], 0) == 0) {
                // Spin
            }
            s_prefix = block_prefixes[logical_idx - 1];
        }

        // Publish: previous prefix + our block total
        block_prefixes[logical_idx] = s_prefix + value;
        __threadfence();  // Ensure prefix visible BEFORE flag
        atomicExch(&block_status[logical_idx], 1);
    }
    __syncthreads();

    // Step 4: All threads add prefix and write
    if (gid < n) {
        output[gid] = s_prefix + value;
    }
}

template<int BLOCK_SIZE>
void ScanChained(int* d_input, int* d_output, int n)
{
    int num_blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;

    int* d_block_prefixes;
    int* d_block_status;
    int* d_block_counter;

    CHECK_CUDA(cudaMalloc(&d_block_prefixes, num_blocks * sizeof(int)));
    CHECK_CUDA(cudaMalloc(&d_block_status, num_blocks * sizeof(int)));
    CHECK_CUDA(cudaMalloc(&d_block_counter, sizeof(int)));

    CHECK_CUDA(cudaMemset(d_block_status, 0, num_blocks * sizeof(int)));
    CHECK_CUDA(cudaMemset(d_block_counter, 0, sizeof(int)));

    ScanChainedKernel<BLOCK_SIZE><<<num_blocks, BLOCK_SIZE>>>(
        d_input, d_output, n, d_block_prefixes, d_block_status, d_block_counter);
    CHECK_CUDA(cudaGetLastError());

    CHECK_CUDA(cudaFree(d_block_prefixes));
    CHECK_CUDA(cudaFree(d_block_status));
    CHECK_CUDA(cudaFree(d_block_counter));
}

===== scan_lookback_single_thread.cuh =====
// scan_lookback_single_thread.cuh

#pragma once

enum class TileStatus: int {
    INVALID = 0,
    AGGREGATE = 1,
    PREFIX = 2
};

union TileDescriptor {
    unsigned long long int raw;
    struct {
        int value;
        TileStatus status;
    };
};

template <int BLOCK_SIZE>
__global__ void ScanDecoupledLookbackSingleThreadKernel(
    const int* __restrict__ input,
    int* __restrict__ output,
    int n,
    TileDescriptor* tile_descriptors,
    int* g_tile_counter)
{
    __shared__ int s_tile_idx;
    __shared__ int s_prefix;

    // Step 1: Claim tile index
    if (threadIdx.x == 0) {
        s_tile_idx = atomicAdd(g_tile_counter, 1);
    }
    __syncthreads();

    int tile_idx = s_tile_idx; // make local copy
    int gid = tile_idx * blockDim.x + threadIdx.x;

    // Step 2: Load and scan block
    int value = (gid < n) ? input[gid] : 0;
    value = BlockScan<BLOCK_SIZE>(value);

    // Step 3: Single thread does the decoupled lookback
    if (threadIdx.x == BLOCK_SIZE - 1) {
        // Publish block aggregate first
        TileDescriptor my_info;
        my_info.value = value;

        if (tile_idx == 0) {
            my_info.status = TileStatus::PREFIX;
        } else {
            my_info.status = TileStatus::AGGREGATE;
        }

        atomicExch(&tile_descriptors[tile_idx].raw, my_info.raw);
        __threadfence();

        if (tile_idx > 0) {
            int lookback_idx = tile_idx - 1;
            int running_prefix = 0;

            // Main lookback loop
            while (lookback_idx >= 0) {
                TileDescriptor pred_info; // predecessor info

                // Spin-wait for valid tile data
                do {
                    pred_info.raw = atomicAdd(&tile_descriptors[lookback_idx].raw, 0);
                } while (pred_info.status == TileStatus::INVALID);

                running_prefix += pred_info.value;

                if (pred_info.status == TileStatus::PREFIX) {
                    break;
                }

                lookback_idx--;
            }

            s_prefix = running_prefix;

            // Upgrade to PREFIX
            my_info.value = s_prefix + value;
            my_info.status = TileStatus::PREFIX;
            atomicExch(&tile_descriptors[tile_idx].raw, my_info.raw);
            __threadfence();
        }
    }
    __syncthreads();

    // Step 4: All threads add prefix and write
    if (gid < n) {
        output[gid] = s_prefix + value;
    }
}

template<int BLOCK_SIZE>
void ScanDecoupledLookbackSingleThread(int* d_input, int* d_output, int n)
{
    int num_blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;

    TileDescriptor* d_tile_descriptors;
    int* d_tile_counter;

    CUDA_CHECK(cudaMalloc(&d_tile_descriptors, num_blocks * sizeof(TileDescriptor)));
    CUDA_CHECK(cudaMalloc(&d_tile_counter, sizeof(int)));

    CUDA_CHECK(cudaMemset(d_tile_descriptors, 0, num_blocks * sizeof(TileDescriptor)));
    CUDA_CHECK(cudaMemset(d_tile_counter, 0, sizeof(int)));

    ScanDecoupledLookbackSingleThreadKernel<BLOCK_SIZE><<<num_blocks, BLOCK_SIZE>>>(
        d_input, d_output, n, d_tile_descriptors, d_tile_counter);
    CUDA_CHECK(cudaGetLastError());

    CUDA_CHECK(cudaFree(d_tile_descriptors));
    CUDA_CHECK(cudaFree(d_tile_counter));
}

===== scan_lookback_warp_coarsen.cuh =====
// scan_lookback_warp_coarsen.cuh
#pragma once

enum class TileStatus: int {
    INVALID = 0,
    AGGREGATE = 1,
    PREFIX = 2
};

union TileDescriptor {
    unsigned long long int raw;
    struct {
        int value;
        TileStatus status;
    };
};

template <int BLOCK_SIZE, int ITEMS_PER_THREAD>
__global__ void ScanDecoupledLookbackWarpCoarsenedKernel(
    const int* __restrict__ input,
    int* __restrict__ output,
    int n,
    TileDescriptor* tile_descriptors,
    int* g_tile_counter)
{
    constexpr int TILE_SIZE = BLOCK_SIZE * ITEMS_PER_THREAD;

    __shared__ int s_tile_idx;
    __shared__ int s_prefix;

    // Step 1: Claim tile index
    if (threadIdx.x == 0) {
        s_tile_idx = atomicAdd(g_tile_counter, 1);
    }
    __syncthreads();

    int tile_idx = s_tile_idx;
    int tile_offset = tile_idx * TILE_SIZE;

    // Step 2: Load ITEMS_PER_THREAD elements per thread
    int items[ITEMS_PER_THREAD];

    #pragma unroll
    for (int i = 0; i < ITEMS_PER_THREAD; i++) {
        int idx = tile_offset + threadIdx.x + i * BLOCK_SIZE;
        items[i] = (idx < n) ? input[idx] : 0;
    }

    // Step 3: Thread-local inclusive scan
    #pragma unroll
    for (int i = 1; i < ITEMS_PER_THREAD; i++) {
        items[i] += items[i - 1];
    }

    // Step 4: BlockScan on thread totals (last item holds thread's sum)
    int thread_total = items[ITEMS_PER_THREAD - 1];
    int thread_prefix_exclusive = BlockScanExclusive<BLOCK_SIZE>(thread_total);

    // Step 5: Add thread prefix to all items
    #pragma unroll
    for (int i = 0; i < ITEMS_PER_THREAD; i++) {
        items[i] += thread_prefix_exclusive;
    }

    // Now items[] holds block-local inclusive scan
    // Block aggregate is last thread's last item
    int block_aggregate = __shfl_sync(0xFFFFFFFF, items[ITEMS_PER_THREAD - 1], BLOCK_SIZE - 1);

    // Step 6: Warp lookback (same as before, just with larger tiles)
    int warp_idx = threadIdx.x / warpSize;
    int lane = threadIdx.x % warpSize;
    constexpr int last_warp_idx = BLOCK_SIZE / warpSize - 1;

    if (warp_idx == last_warp_idx) {
        // Publish aggregate
        if (lane == warpSize - 1) {
            TileDescriptor my_info;
            my_info.value = block_aggregate;
            my_info.status = (tile_idx == 0) ? TileStatus::PREFIX : TileStatus::AGGREGATE;
            atomicExch(&tile_descriptors[tile_idx].raw, my_info.raw);
            __threadfence();
        }
        __syncwarp();

        if (tile_idx == 0) {
            s_prefix = 0;
        } else {
            int exclusive_prefix = 0;
            int lookback_base = tile_idx - 1;

            while (true) {
                int my_lookback_idx = lookback_base - lane;

                TileDescriptor pred_info;
                pred_info.value = 0;
                pred_info.status = TileStatus::PREFIX;

                if (my_lookback_idx >= 0) {
                    do {
                        pred_info.raw = atomicAdd(&tile_descriptors[my_lookback_idx].raw, 0);
                    } while (pred_info.status == TileStatus::INVALID);
                }

                unsigned prefix_mask = __ballot_sync(0xFFFFFFFF, pred_info.status == TileStatus::PREFIX);
                int prefix_lane = __ffs(prefix_mask) - 1;

                int my_contribution = (lane <= prefix_lane) ? pred_info.value : 0;

                for (int offset = 1; offset < warpSize; offset *= 2) {
                    int other = __shfl_up_sync(0xFFFFFFFF, my_contribution, offset);
                    if (lane >= offset) {
                        my_contribution += other;
                    }
                }

                int iteration_sum = __shfl_sync(0xFFFFFFFF, my_contribution, prefix_lane);
                exclusive_prefix += iteration_sum;

                if (prefix_lane < warpSize - 1 || my_lookback_idx <= 0) {
                    break;
                }

                lookback_base -= warpSize;
            }

            if (lane == 0) {
                s_prefix = exclusive_prefix;
            }

            if (lane == warpSize - 1) {
                TileDescriptor my_info;
                my_info.value = exclusive_prefix + block_aggregate;
                my_info.status = TileStatus::PREFIX;
                atomicExch(&tile_descriptors[tile_idx].raw, my_info.raw);
                __threadfence();
            }
        }
    }
    __syncthreads();

    // Step 7: Add global prefix and write
    #pragma unroll
    for (int i = 0; i < ITEMS_PER_THREAD; i++) {
        int idx = tile_offset + threadIdx.x + i * BLOCK_SIZE;
        if (idx < n) {
            output[idx] = s_prefix + items[i];
        }
    }
}

template<int BLOCK_SIZE>
void ScanDecoupledLookbackWarpCoarsen(int* d_input, int* d_output, int n)
{
    int num_blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;

    TileDescriptor* d_tile_descriptors;
    int* d_tile_counter;

    CUDA_CHECK(cudaMalloc(&d_tile_descriptors, num_blocks * sizeof(TileDescriptor)));
    CUDA_CHECK(cudaMalloc(&d_tile_counter, sizeof(int)));

    CUDA_CHECK(cudaMemset(d_tile_descriptors, 0, num_blocks * sizeof(TileDescriptor)));
    CUDA_CHECK(cudaMemset(d_tile_counter, 0, sizeof(int)));

    ScanDecoupledLookbackWarpCoarsenKernel<BLOCK_SIZE><<<num_blocks, BLOCK_SIZE>>>(
        d_input, d_output, n, d_tile_descriptors, d_tile_counter);
    CUDA_CHECK(cudaGetLastError());

    CUDA_CHECK(cudaFree(d_tile_descriptors));
    CUDA_CHECK(cudaFree(d_tile_counter));
}

===== scan_lookback_warp.cuh =====
// scan_lookback_warp.cuh
#pragma once

enum class TileStatus: int {
    INVALID = 0,
    AGGREGATE = 1,
    PREFIX = 2
};

union TileDescriptor {
    unsigned long long int raw;
    struct {
        int value;
        TileStatus status;
    };
};

template <int BLOCK_SIZE>
__global__ void ScanDecoupledLookbackWarpKernel(
    const int* __restrict__ input,
    int* __restrict__ output,
    int n,
    TileDescriptor* tile_descriptors,
    int* g_tile_counter)
{
    __shared__ int s_tile_idx;
    __shared__ int s_prefix;

    // Step 1: Claim tile index
    if (threadIdx.x == 0) {
        s_tile_idx = atomicAdd(g_tile_counter, 1);
    }
    __syncthreads();

    int tile_idx = s_tile_idx;
    int gid = tile_idx * BLOCK_SIZE + threadIdx.x;

    // Step 2: Load and scan block
    int value = (gid < n) ? input[gid] : 0;
    value = BlockScan<BLOCK_SIZE>(value);

    // Step 3: Last warp does the decoupled lookback
    int warp_idx = threadIdx.x / warpSize;
    int lane = threadIdx.x % warpSize;
    constexpr int last_warp_idx = BLOCK_SIZE / warpSize - 1;

    if (warp_idx == last_warp_idx) {
        // Get block aggregate from last thread's value
        int block_aggregate = __shfl_sync(0xFFFFFFFF, value, warpSize - 1);

        // Publish aggregate (only one thread writes)
        if (lane == warpSize - 1) {
            TileDescriptor my_info;
            my_info.value = block_aggregate;
            my_info.status = (tile_idx == 0) ? TileStatus::PREFIX : TileStatus::AGGREGATE;
            atomicExch(&tile_descriptors[tile_idx].raw, my_info.raw);
            __threadfence();
        }
        __syncwarp();

        if (tile_idx == 0) {
            s_prefix = 0;
        } else {
            int exclusive_prefix = 0;
            int lookback_base = tile_idx - 1;  // Closest predecessor

            while (true) {
                int my_lookback_idx = lookback_base - lane;

                // Each lane spins on its own predecessor
                TileDescriptor pred_info;
                pred_info.value = 0;
                pred_info.status = TileStatus::PREFIX;  // Default for out-of-bounds

                if (my_lookback_idx >= 0) {
                    do {
                        pred_info.raw = atomicAdd(&tile_descriptors[my_lookback_idx].raw, 0);
                    } while (pred_info.status == TileStatus::INVALID);
                }

                // Which lanes found PREFIX?
                unsigned prefix_mask = __ballot_sync(0xFFFFFFFF, pred_info.status == TileStatus::PREFIX);

                // Find closest PREFIX (lowest lane = highest lookback_idx)
                int prefix_lane = __ffs(prefix_mask) - 1;

                // Sum values from lane 0 through prefix_lane using inclusive scan
                int my_contribution = (lane <= prefix_lane) ? pred_info.value : 0;

                // Warp inclusive scan
                for (int offset = 1; offset < warpSize; offset *= 2) {
                    int other = __shfl_up_sync(0xFFFFFFFF, my_contribution, offset);
                    if (lane >= offset) {
                        my_contribution += other;
                    }
                }

                // prefix_lane has sum of lanes 0..prefix_lane
                int iteration_sum = __shfl_sync(0xFFFFFFFF, my_contribution, prefix_lane);
                exclusive_prefix += iteration_sum;

                // If we hit a PREFIX, we're done
                if (prefix_lane < warpSize - 1 || (my_lookback_idx <= 0 && lane == prefix_lane)) {
                    // Either found PREFIX mid-warp, or reached tile 0
                    break;
                }

                // All 32 were AGGREGATEâ€”shift back and continue
                lookback_base -= warpSize;
            }

            if (lane == 0) {
                s_prefix = exclusive_prefix;
            }

            // Upgrade to PREFIX
            if (lane == warpSize - 1) {
                TileDescriptor my_info;
                my_info.value = exclusive_prefix + block_aggregate;
                my_info.status = TileStatus::PREFIX;
                atomicExch(&tile_descriptors[tile_idx].raw, my_info.raw);
                __threadfence();
            }
        }
    }
    __syncthreads();

    // Step 4: All threads add prefix and write
    if (gid < n) {
        output[gid] = s_prefix + value;
    }
}

template<int BLOCK_SIZE>
void ScanDecoupledLookbackWarp(int* d_input, int* d_output, int n)
{
    int num_blocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;

    TileDescriptor* d_tile_descriptors;
    int* d_tile_counter;

    CUDA_CHECK(cudaMalloc(&d_tile_descriptors, num_blocks * sizeof(TileDescriptor)));
    CUDA_CHECK(cudaMalloc(&d_tile_counter, sizeof(int)));

    CUDA_CHECK(cudaMemset(d_tile_descriptors, 0, num_blocks * sizeof(TileDescriptor)));
    CUDA_CHECK(cudaMemset(d_tile_counter, 0, sizeof(int)));

    ScanDecoupledLookbackWarpKernel<BLOCK_SIZE><<<num_blocks, BLOCK_SIZE>>>(
        d_input, d_output, n, d_tile_descriptors, d_tile_counter);
    CUDA_CHECK(cudaGetLastError());

    CUDA_CHECK(cudaFree(d_tile_descriptors));
    CUDA_CHECK(cudaFree(d_tile_counter));
}

===== scan_multi_kernel.cuh =====
// scan_multi_kernel.cuh
// multi kernel implementation should go here

===== utils.cuh =====
// utils.cuh
#pragma once

#define CHECK_CUDA(val) check((val), #val, __FILE__, __LINE__)
void check(cudaError_t err, const char* const func, const char* const file,
           const int line)
{
    if (err != cudaSuccess)
    {
        std::cerr << "CUDA Error at: " << file << ":" << line << std::endl;
        std::cerr << cudaGetErrorString(err) << " " << func << std::endl;
        std::exit(EXIT_FAILURE);
    }
}

===== warp_scan.cuh =====
// warp_scan.cuh

static __device__ __forceinline__ int WarpScan(int value)
{
    int lane = threadIdx.x % warpSize;
    for (int offset = 1; offset < warpSize; offset *= 2) {
        int tmp = __shfl_up_sync(0xFFFFFFFF, value, offset);
        if (lane >= offset) {
            value += tmp;
        }
    }
    return value;
}